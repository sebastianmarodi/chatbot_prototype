{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medi-Guide \n",
    "## A Langchain Framework-Based NLP Chatbot Prototype Using Open AI API and RAG on The Kegg Medicus Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medi-Guide uses `Retrieval Augmented Generation` on the [Kegg Medicus Database](https://www.genome.jp/kegg/medicus.html#:~:text=KEGG%20MEDICUS%20is%20a%20health,in%20Japan%20and%20the%20USA.) to give users accuracte and scientifically valid information on medicines and drug interactions as well as in-depth information about disease and the human genome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kegg Medicus Database has information that is relevant for the following industries:\n",
    "\n",
    "1. Pharmaceutical Industry: \n",
    "    The database includes information on various drugs and their efficacy in treating specific conditions \n",
    "    such as rheumatoid arthritis and cancer. Pharmaceutical companies can use this information for research \n",
    "    and development of new drugs or improving existing ones.\n",
    "\n",
    "2. Biotechnology Industry: \n",
    "    The database contains information on genes, variations, and signaling pathways related to diseases such \n",
    "    as hepatocellular carcinoma. Biotech companies can utilize this information for developing targeted \n",
    "    therapies or diagnostic tools.\n",
    "\n",
    "3. Healthcare Industry: \n",
    "    The database includes information on drugs used for antihypertensive and vasodilator purposes. \n",
    "    Healthcare providers can use this information to better understand the efficacy and potential \n",
    "    side effects of these drugs for patient treatment.\n",
    "\n",
    "4. Research Institutions: \n",
    "    The database provides valuable information on various drugs, their mechanisms of action, and their \n",
    "    potential applications. Research institutions can use this information for conducting further studies \n",
    "    and advancing scientific knowledge in the field of medicine.\n",
    "\n",
    "This is not an exhaustive list, and other industries or sectors may also find\n",
    "value in the information contained in the database depending on their specific needs and interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chat agent generates responses to prompts by engaging in a process of called `chain of thought resoning`.  Chain of though reasoning selects appropriate tools connected to the agent in order to make a decision on how best to approach generating an output. \n",
    "\n",
    "The agent has three tools, namely:\n",
    "- The Kegg Medicus Vector Database, Hosted via Pinecone.io\n",
    "- Web Search on MD.com\n",
    "- Agent Memory Summarization Tool\n",
    "\n",
    "These tools, provide the context for the conversational responses. \n",
    "\n",
    "The agent is customized via a system prompt that serves as a guardrail against discussing anything besides medical topics or giving advice that could be harmful to users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    " * Dependencies\n",
    " * Chat Model\n",
    " * Document Loader\n",
    " * Text Splitter\n",
    " * Data Storage\n",
    " * Output Generation / Completion\n",
    " * Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai langchain tiktoken faiss-cpu python-dotenv pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "def config():\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "config()\n",
    "llm = ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "                 temperature=0.0, \n",
    "                 model_name='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken \n",
    "tiktoken.encoding_for_model('gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(example_texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"kegg-medicus-database-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katle\\anaconda3\\envs\\dl_env\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone  \n",
    "import tqdm    \n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key=os.getenv(\"PINECONE_API_KEY\"), \n",
    "\tenvironment=os.getenv(\"PINECONE_ENV\")          \n",
    ")      \n",
    "index = pinecone.Index('kegg-medicus-database-index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index=index, \n",
    "    embedding=embed, #.embed_query(), \n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.19462,\n",
       " 'namespaces': {'': {'vector_count': 19462}},\n",
       " 'total_vector_count': 19462}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q & A Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flavin adenine dinucleotide (FAD) is a redox-active coenzyme associated with various proteins, which is involved in several important enzymatic reactions in metabolism. FAD can exist in two different redox states: the oxidized form, FAD, and the reduced form, FADH2. It is composed of two main parts: a riboflavin moiety (the flavin) and an adenosine diphosphate (ADP) moiety. These two parts are connected through a phosphate group.\\n\\nFAD plays a crucial role in the process of cellular respiration, where it acts as a hydrogen carrier, accepting electrons during the oxidation of substrates and then transferring them to the electron transport chain in mitochondria, where ATP is produced. FAD-dependent enzymes are involved in various metabolic pathways, including the citric acid cycle (Krebs cycle) and fatty acid oxidation.\\n\\nIn the context of the electron transport chain, FADH2 donates electrons to complex II (succinate dehydrogenase), which then passes the electrons through a series of carriers to ultimately reduce molecular oxygen to water, a process coupled with the synthesis of ATP.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Flavin adenine dinucleotide?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duck Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "def duck_wrapper(input_text):\n",
    "    try:\n",
    "        search_results = search.run(f'''site:medlineplus.gov {input_text}''') \n",
    "    except Exception as er:\n",
    "        print(er)\n",
    "        return \"There was an error fetching results for that query. Please try again\"\n",
    "    # print(search_results)\n",
    "    else:\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There was an error fetching results for that query. Please try again'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"foods for better focus\"\n",
    "duck_wrapper(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "mem_template = \"\"\"This is a conversation between a human and a bot:\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Write a summary of the conversation for {input}:\n",
    "\"\"\"\n",
    "\n",
    "mem_prompt = PromptTemplate(input_variables=[\"input\", \"chat_history\"], template=mem_template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "summary_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=mem_prompt,\n",
    "    verbose=True,\n",
    "    memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Medicus Text Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            '''use this tool to respond to queries about drugs (medicine) and drugs interactions for (contraindications (CI) and precautions (P)),\n",
    "            disease and the human genome'''\n",
    "        )\n",
    "    ), \n",
    "    Tool(\n",
    "        name ='Web Search',\n",
    "        func=duck_wrapper,\n",
    "        description=(\n",
    "            '''use this tool to answer more general questions about health and wellness\n",
    "            '''\n",
    "        )\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Summary\",\n",
    "        func=summary_chain.run,\n",
    "        description=\"useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "tool_names = [tool.name for tool in tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent\n",
    "\n",
    "prefix = \"\"\"Be convrersational and act as a smart expert medical advisor. Answering question as best as YOU can. \n",
    "You have access to the following tools:\"\"\"\n",
    "\n",
    "suffix = \"\"\"Begin!\"\n",
    "\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello\" \n",
    "agent_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"I have flu, can ginger cure me?\" \n",
    "agent_chain(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"What properties in Ginger give it its anti-inflammatory and antioxidant properties?\"\n",
    "agent_chain(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"What other plants or medicines have similar benefits?\"\n",
    "agent_chain(query4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = \"You have acess to the database as a tool provide to you as an retrieval tool.\"\n",
    "agent_chain(query5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(query):\n",
    " return agent_chain(query)['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_response(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
